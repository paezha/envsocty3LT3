---
title: "Reading 2: Geospatial Visualization for Transportation Trends using R"
author: 
- name: Antonio PÃ¡ez
- name: My Name 
subject: "ENVSOCTY 3LT3 Transportation Geography"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    # The project-template-default.tex file was heavily  adapted from Steven V. Miller's template for academic manuscripts. See:
    # http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/
    # https://github.com/svmiller/svm-r-markdown-templates/blob/master/svm-latex-ms.tex
    template: reading-template-default.tex
always_allow_html: true
---

# Introduction

NOTE: This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Transportation is a spatial phenomenon. It is natural then that we would be interested in studying it from a spatial perspective. In recent years, this has become increasingly possible, thanks to 1) the availability of information that is geo-coded, in other words, that has geographical references; and 2) the availability of software to analyze such information.

A key technology fuelling this trend is that of Geographical Information Systems (GIS). GIS are, at their simplest, digital mapping for the 21st century. In most cases, however, GIS go beyond cartographic functions to also enable and enhance our ability to analyze data.

There are many available packages for geographical information analysis. Some are very user friendly, and widely available in many institutional contexts, such as ESRI's Arc software. Others are fairly specialized, such as Caliper's TransCAD, which implements many operations of interest for transportation engineering and planning. Others have the advantage of being more flexible and/or free.

Such is the case of the R statistial computing language. `R` has been adopted by many in the geographical analysis community, and a number of specialized libraries have been developed to support mapping and spatial data analysis functions.

The objective of this note is to provide an introduction to the use of R for geospatial visualization.

To use this note you will need the following:

* This R markdown notebook.
* A zip file provided with the reading, that contains the following:
    + A set of files (shape) called `Hamilton CMA tts06`
    + An Excel file called `Hamilton CMA Trips by Purpose`

The data used for this tutorial were retrieved from the 2011 Transportation Tomorrow Survey [TTS](http://www.transportationtomorrow.on.ca/), the periodic travel survey of the Greater Toronto and Hamilton Area, as well as data from the 2011 Canadian Census [Census Program](http://www12.statcan.gc.ca/census-recensement/index-eng.cfm).

# Loading geographic files

The most basic task when doing digital mapping is working with geographical files.

There are many different formats for geographic files, depending on whether they are vectors (i.e., line-based) or raster (i.e., pixel-based), and also by different developers. A library (`rgdal`) has been developed to support reading and working with many different types of geospatial files.

A common type of file, which has to some extent become the de-facto universal format is ESRI's shape. A shape "file" is in fact a collection of files that include geospatial as well as alphanumeric information.

We'll begin this tutorial by loading and visualizing a shape file.

Before starting, however, it is useful to make sure that you have a clean Environment. The following command will remove all data and values currently in the Environment.
```{r}
rm(list = ls())
```

Next, you must load the libraries needed for analysis (you may need to install them before they are available for loading):
```{r}
#library(rgdal)
#library(readxl)
#library(dplyr)
#library(ggplot2)
#library(tmap)
#library(ggmap)
#library(broom)
#library(readr)
#library(maptools)
library(envsocty3LT3) # Course package
library(sf)
library(tidyverse)
```

We now proceed to see how a geo-spatial file can be read. 

For this example, you will use an ESRI shape file that contains the Traffic Analysis Zones for the Hamilton Census Metropolitan Area (CMA) (retrieved from [here](http://dmg.utoronto.ca/survey-boundary-files#2006_zone)). The command used for this is `sf::st_read` (note that `package::function` indicates a function in the name package). 

To read the file, *you need to navigate to the directory where the files are and make it your working directory*. To assign the file to an object, use the assignment `<-` and give the object a name; in this case `taz`, short for "traffic analyisis zones":
```{r}
data("hamilton_taz")
```

This assignment will create an `sf` table, an R object used to store geographical information (in this example spatial polygons; other types of spatial data are points, lines). Objects of the class `sf` use the Simple Features standard for representing geometry.

Traffic Analysis Zones (or TAZ) are a form of zoning system typically used for the analysis of transportation systems. TAZs are typically designed to contain a relatively small number of households, hopefully with relatively homogenous socio-economic and/or demographic characteristics. Currently, the table includes just some identifiers and the geometry of the zones. You can verify this by means of the `head` function:
```{r}
head(taz)
```

You can quickly plot this object using `ggplot2`:

```{r}
plot(taz)
```

The command _plot_, by default, will produce a tiled set of maps for the object, with each of the maps being a thematic representation of one column of data. For a plot of one specific variable, you specify that column as below:

```{r}
plot(taz["AREA"])
```

To plot only the geometry of this map, without any variables, use the function `st_geometry`: it will ignore other attributes in the table. We will use other plotting strategies later on, but for the moment this is enough to quickly visualize `taz` as a map:

```{r}
plot(st_geometry(taz))
```

As you saw above, this `sf` dataframe contains some identifiers, including "GTA06", which is the unique identifier for the traffic analysis zones. This identifier was saved as a number, but we need later on to use it as a character, so first we will change it to character. This can be done by means of the functions `mutate` and `as.character`. `as.character` will take a variable and will convert it to characters, whereas `mutate` changes the contents of the table:

```{r}
taz <- mutate(taz, GTA06 = as.character(GTA06))
```

There is not much information in this table, and the table therefore is not particularly interesting. If more data are available, they can be _joined_ to the table for further analysis, as you will see next.

Before proceeding, it is important to note there are many other different packages in R that can be used to visualize geospatial information, each with strenghts and weaknesses. Such packages are:

* `ggplot2`: used for 2 dimensional plots; you used it before when you read `01 Overview of Data Analysis and Visualization.Rmd`
* `tmap`: used to create thematic maps.
* `ggmap`: a package for static maps that works on similar principles as ggplot2.
* `leaflet`: used to create dynamic maps.

# Joining data to an `sf` object

The geographical file, and map, that you produced above are not terribly interesting, since they contain little more than the geometry of the zones. Fortunately, we have additional information in a separate Excel file that can help us make this map more interesting. This file can be read using the `read_excel` command:
```{r}
travel_data <- read_excel("Hamilton CMA Trips by Purpose.xlsx")
```

The above loads a data frame called `travel_data`. This data frame includes information drawn from the 2011 Transportation Tomorrow Survey (TTS), on number of trips for different purposes that originate at each TAZ, and information from the Canadian Census of 2011. (for information on the TTS check the [data guide](http://www.dmg.utoronto.ca/pdf/tts/2011/dataguide2011.pdf)).

The TTS columns in the data frame are number of trips by purpose (e.g., Work trips, School trips, Market and Shop trips, etc.), in addition to an identifier GTA06, which is identical to the GTA06 column in the `taz` dataframe.

The Census columns include for each TAZ the population, number of full time and part time workers, number of people who worked at home, population density, median age of population, number of families of different sizes, median income, average income, employment and unemployment rates, and median commuting duration in minutes. 

To join two tables, the set of `join` commands from the `dplyr` package are useful. In this case, we want to execute a _left_join_. A _left_join_ takes all the rows from table x (to the left), and adds to that all the columns of table y for those rows. You can join the data frame `travel_data` to `taz` as follows:
```{r}
taz <- left_join(taz, travel_data)
```

As you can see, _left_join_ looks for a column name that exists in both files, so that it can join the data across. In this case, it found GTA06: we had to coerce GTA06 to character, above, because the travel-data file has GTA06 data saved as characters.

It is possible to conduct operations using the data frame with the SpatialPolygonDataFrame characteristic. For instance, let's say that you would like to calculate a per capita measure of mobility, say trips per capita, for a specific purpose. This can be done using the number of work trips and the population, and the function `mutate`:

```{r}
taz <- mutate(taz, work.pc = Work / Population)
```

(If you remember the previous lab, we would have done the same work with

taz$work.pc <- taz$Work / taz$Population

This illustrates that there are several different ways to do the same compuation work in R.)

You can use more complex formulas too. For instance, for each zone we can find the proportion of work trips relative to the total number of trips produced by each zone:

```{r}
taz <- mutate(taz, work.prop = Work / (Unknown + Subsequent_School + Daycare + Facilitate_passenger + Home + Market_Shop + Other + Subsequent_Work + School + Work))
```

And you can query this table. For instance, you can find how many zones produce 646 work trips or more:

```{r}
sum(taz$Work > 646)
```

# More on regressions

You can even use a chart to see the distribution of a variable across different categories. For example, here is how to plot a density chart, with multiple lines for multiple characteristics:

```{r}
# make a density plot showing the difference in income distribution
# for different unemployment rates
a <- ggplot(taz, aes(x = Median_income))
a + geom_density(aes(color = as.factor(Unemployment_rate > 7)))
```

The above chart, a comparative density plot, is rather like a histogram, just with a curve instead of bars. It plots the median incomes (x axis) for every TAZ in the dataset, after first splitting the dataset into TAZes with high unemployment and TAZes with low unemployment. The result is a chart that shows the relative prevalence of one characteristic as another characteristic varies: in this case, it shows how prevalent each median income is across TAZes.

In this case, you can see that a TAZ with an unemployment rate below 7% (the red line) will generally have a much higher median income than a TAZ with an unemployment rate above 7% (the blue line): the most prevalent median income for a high-unemployment TAZ is around 30,000 dollars, while the most prevalent median income for a low-unemployment TAZ is about 10,000 higher. This is a significant difference.

When you see a chart like this, it suggests there's a relationship between median income and unemployment that you could study with a regression analysis:

```{r}
e <- lm(formula = Median_income ~ Unemployment_rate, data = taz)
summary(e)
```

Obviously, the higher an area's unemployment is, the less is its median income. The regression analysis confirms this: for every 1% increase in unemployment rate, a TAZ's median income drops by about 1176 dollars.

But what about that neighbourhood's proportion of trips for work, which we calculated above? Does that vary by median income as well?

```{r}
summary(taz$work.prop)
```

The median proportion of trips for work is about 0.17, so let's use that for our chart's cutoff.

```{r}
# make a density plot showing the difference in income distribution
# for different unemployment rates
b <- ggplot(taz, aes(x = Median_income))
b + geom_density(aes(color = as.factor(work.prop > 0.17)))
```

It might be that areas with a higher median income generate a higher proportion of trips for work, according to the above chart plot: areas with a higher proportion of work trips (in blue) are shifted slightly to the right in median income. But the way that this chart is shaped, it suggests that something else is going on.

If we run a regression on work trips proportion versus income, can we see anything of significance?

```{r}
f <- lm(formula = work.prop ~ Median_income, data = taz)
summary(f)
```

The p-value of 0.0003 for the regression shows that there is certainly some definite relationship between proportion of trips for work and median income.

However, the R-squared of 0.04 shows that it's only a very insignificant determinant of number of work trips. An R-squared of 0.04 means that the independent variable (Median Income, in this case) is only explaining a tiny amount (4 percent) of the variation in the dependent variable (work proportion, in this case). In fact, the value for the coefficient of median income (3.016*10^-6, or 0.000003016) suggests that the number of work trips only increases by 0.3016 for every ten thousand dollars increase in median income - that's not a lot.

But we did learn, above, that an area's unemployment rate strongly affects its median income; that might mean that an area's unemployment rate also affects its proportion of work trips. It certainly seems logical that the more unemployment in an area, the fewer trips for work will occur. But what does a regression say?

We can do a regression of a dependent variable on two or more independent variables, not just on one:

```{r}
g <- lm(formula = work.prop ~ Median_income + Unemployment_rate, data = taz)
summary(g)
```

The p-value of 0.46 for unemployment rate, above, indicates that an area's unemployment rate has no statistically significant effect on the proportion of work trips, after you take into account the area's median income! How is this possible? 

There is probably some explanation. Perhaps the employed people with in areas with low median income have to take more trips; perhaps TAZes with lower median income (and higher unemployment rate generally) see more employed people working part-time jobs that have shorter hours, and so they have to make more work trips per day as they travel between jobs. 

But why did significance of unemployment rate disappear the moment we added median income to our regression? This has to do with a concept called _collinearity_. We proved above that unemployment rate and median income are _negatively correlated_, meaning that as a neighbourhood's unemployment rate goes up, its median income goes down. One effect of this is, if you regress on only the median income variable, you are seeing the effect of change in median income plus the change in unemployment rate, even where you don't include unemployment in your regression! This is a very nasty problem with regressions: isolating the effect of one variable is hard.

The point of the above, though, was simply to show you that data exploration can lead you to interesting results: upon seeing the above data, we might want to do a study on the topic and write a paper. 

Also, we wanted to show you that that you can do a regression on multiple variables, not just on one.

# Creating thematic maps

Once a `sf` dataframe has interesting information, it is possible to create thematic maps. If we wanted, we could still work with the _plot_ command:

```{r}
plot(taz["Median_income"])
```

But we can also use more powerful plotting commands. Let's try the package `ggplot2` for this purpose.

As before, `ggplot2` objects are created by layering. We begin by creating an empty `ggplot2` object as follows:

```{r}
ggplot(data = taz)
```

It is blank because we have not yet told the package to actually draw anything. The function for rendering `sf` objects is `geom_sf`. We can plot the geometry only as follows:
```{r}
ggplot(data = taz) +
  geom_sf()
```

The default settings of `qtm` use a gray style for the polygons and add a frame to the map. Thematic maps are created by mapping some aesthetic value of the plot to a variable. For instance, we can map the aestethic `fill` to `Work`:
```{r}
ggplot(data = taz) +
  geom_sf(aes(fill = Work))
```

We can also change other aesthetics without mapping them to a variable (so we put them outside of `aes()`). For example, we can change the color of the lines to "white":

```{r}
ggplot(data = taz) +
  geom_sf(aes(fill = Work), color = "white")
```

When we use a variable to fill the polygons, like the above median income maps, we are creating a choropleth map (a map where the colors are selected based on the values of an underlying variable). This creates a _statistical map_. The map also includes a legend that can be used to interpret the coloring of the map.

The default coloring scheme was for a continuous variable. As you can see above, the presence of a few zones with a large number of trips obscures somewhat the distribution of values elsewhwere. This reduces the usefulness of the map, and you might want to change this by using a different coloring scheme. For instance, using the `cut()` function it is possible to categorize the variable in quantiles. The simplest quantile is when we divide the sample in two. For this we would indicate that the cuts are done at _breaks_ 0, 0.5, and 1, or in other words at 0%, 50%, and 100% of the sample:
```{r}
Work_breaks <- quantile(taz$Work, c(0, 0.5, 1))

ggplot(data = taz) +
  geom_sf(aes(fill = cut(Work, Work_breaks)))
```

Or we could use a finer set of breaks, say at 0%, 20%, 40%, 60%, 80%, 100% (these are called _quintiles_, they divide the sample in five equal parts):
```{r}
Work_breaks <- quantile(taz$Work, c(0, 0.2, 0.4, 0.6, 0.8, 1))

ggplot(data = taz) +
  geom_sf(aes(fill = cut(Work, Work_breaks)))
```

We can improve the map above in many ways, but for the moment, it is useful to change the coloring scheme to better represent the way values change from less to more. We do this by means of the function `scale_fill_brewer()` and selecting _sequential_ color palette for the plot:
```{r}
ggplot(data = taz) +
  geom_sf(aes(fill = cut(Work, Work_breaks))) + 
  scale_fill_brewer(type = "seq")
```

The map above map is easier to interpret from a statistical standpoint, because it shows that 20% of the zones that generate the least trips generate between zero and 158 trips each. The second quintile are zones that generate between 158 and 493 trips, and so on. 

Congratulations! Now you can do thematic mapping using `R`.

# Bonus Material

The advantage of using a programmatic approach to visualization (i.e., programming your maps) is that everything is documented, it makes it easy to reproduce the maps, and it also gives you very fine control over the aspect of your maps. An advanced example is below (do not worry about understanding the code below, it is just to illustrate the possibilities):

```{r}
Work_breaks <- quantile(taz$Work, c(0, 0.2, 0.4, 0.6, 0.8, 1))

ggplot(data = taz) +
  geom_sf(aes(fill = cut(Work, Work_breaks))) + 
  scale_fill_brewer(type = "seq", labels = c("<158", "159-493", "494-840", "840-1,240", ">1,241")) +
  theme_minimal() +
  labs(fill = "Work Trips (Quintiles)") +
  theme(legend.position = "bottom")
```

